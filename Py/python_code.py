# # -*- coding: utf-8 -*-
# """Python Code

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/19N4RxSZ5Oap-2jw-gMy3rFsk6sQRMfzR

# **Online Jupyter for Python Code**
# """

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score, mean_squared_error
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from itertools import combinations
from sklearn.decomposition import PCA

dataset = pd.read_csv("assets/heart_attack.csv")
# dataset.head()

# Logistic Regression Code for Preliminary Inputs

def LogisticRegPre(age, gender, trestbps, has_history, cp):
    X = dataset[['age', 'gender', 'trestbps', 'has_history', 'cp']]
    y = dataset['heart_disease']

    X_df = pd.DataFrame(X, columns=['age','gender','trestbps','has_history','cp'])
    # Standardize the features
    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X_df)

    X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

    # Create a new logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp]],
                                columns = ['age','gender','trestbps','has_history','cp'])

    user_features_standardized = scaler.transform(user_features)

    # Predictions on the scaled test set
    predictions = model.predict(user_features_standardized)
    probabilities = model.predict_proba(user_features_standardized)[0, 1]
    new_patient_data = np.array([age, gender, trestbps, has_history, cp]).reshape(1, -1)
    new_patient_scaled = scaler.transform(new_patient_data)
    probability = round(model.predict_proba(new_patient_scaled)[0, 1]*100,4)

    y_pred_test = model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predictions[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# Logistic Regression Code for Above 35 Probability

def LogisticRegAbove35(age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal):
    X = dataset[['age', 'gender', 'trestbps', 'has_history', 'cp','chol', 'fbs', 'restecg', 'thalach', 'thal']]
    y = dataset['heart_disease']

    X_df = pd.DataFrame(X, columns=['age','gender','trestbps','has_history','cp','chol', 'fbs', 'restecg', 'thalach', 'thal'])
    # Standardize the features
    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X_df)

    X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

    # Create a new logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal]],
                                columns = ['age','gender','trestbps','has_history','cp','chol', 'fbs', 'restecg', 'thalach', 'thal'])

    user_features_standardized = scaler.transform(user_features)

    # Predictions on the scaled test set
    predictions = model.predict(user_features_standardized)
    probabilities = model.predict_proba(user_features_standardized)[0, 1]
    new_patient_data = np.array([age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal]).reshape(1, -1)
    new_patient_scaled = scaler.transform(new_patient_data)
    probability = round(model.predict_proba(new_patient_scaled)[0, 1]*100,4)

    y_pred_test = model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predictions[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# Logistic Regression Code for Below 35 Probability

def LogisticRegBelow35(age, gender, trestbps, has_history, cp, chol, fbs, restecg):
    X = dataset[['age', 'gender', 'trestbps', 'has_history', 'cp','chol', 'fbs', 'restecg']]
    y = dataset['heart_disease']

    X_df = pd.DataFrame(X, columns=['age','gender','trestbps','has_history','cp','chol', 'fbs', 'restecg'])
    # Standardize the features
    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X_df)

    X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)

    # Create a new logistic regression model
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg]],
                                columns = ['age','gender','trestbps','has_history','cp','chol', 'fbs', 'restecg'])

    user_features_standardized = scaler.transform(user_features)

    # Predictions on the scaled test set
    predictions = model.predict(user_features_standardized)
    probabilities = model.predict_proba(user_features_standardized)[0, 1]
    new_patient_data = np.array([age, gender, trestbps, has_history, cp, chol, fbs, restecg]).reshape(1, -1)
    new_patient_scaled = scaler.transform(new_patient_data)
    probability = round(model.predict_proba(new_patient_scaled)[0, 1]*100,4)

    y_pred_test = model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predictions[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# logregdatasetabove = dataset[['age', 'gender', 'trestbps', 'has_history', 'cp','chol', 'fbs', 'restecg', 'thalach', 'thal','heart_disease']].copy()
# sns.pairplot(logregdatasetabove, hue='heart_disease')
# plt.show()

# logregdatasetbelow = dataset[['age', 'gender', 'trestbps', 'has_history', 'cp','chol', 'fbs', 'restecg', 'thalach', 'thal','heart_disease']].copy()
# sns.pairplot(logregdatasetbelow, hue='heart_disease')
# plt.show()

# K-Nearest Code for Preliminary Inputs

def KNNPre(age, gender, trestbps, has_history, cp):
    X = dataset[['age','gender','trestbps','has_history','cp']]
    y = dataset['heart_disease']

    k=12

    X_train, X_test, y_train, y_test = train_test_split(X, y
                                                    , test_size=0.2
                                                    , random_state=42)
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp]],
                                columns = ['age','gender','trestbps','has_history','cp'])

    prediction = knn.predict(user_features)

    predicted_class = dataset['heart_disease'][prediction].values[0]
    probabilities = knn.predict_proba(user_features)

    probability = round(probabilities[0][1]*100, 4)

    y_pred_test = knn.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predicted_class, probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse  

# K-Nearest Code for Above 35 Probability

def KNNAbove35(age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal):
    X = dataset[['age','gender','trestbps','has_history','cp','chol','fbs','restecg','thalach','thal']]
    y = dataset['heart_disease']

    k=12

    X_train, X_test, y_train, y_test = train_test_split(X, y
                                                    , test_size=0.2
                                                    , random_state=42)
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal]],
                                columns = ['age','gender','trestbps','has_history','cp','chol','fbs','restecg','thalach','thal'])

    prediction = knn.predict(user_features)

    predicted_class = dataset['heart_disease'][prediction].values[0]
    probabilities = knn.predict_proba(user_features)

    probability = round(probabilities[0][1]*100, 4)

    y_pred_test = knn.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predicted_class, probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# K-Nearest Code for Below 35 Probability

def KNNBelow35(age, gender, trestbps, has_history, cp, chol, fbs, restecg):
    X = dataset[['age','gender','trestbps','has_history','cp','chol','fbs','restecg']]
    y = dataset['heart_disease']

    k=12

    X_train, X_test, y_train, y_test = train_test_split(X, y
                                                    , test_size=0.2
                                                    , random_state=42)
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)

    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg]],
                                columns = ['age','gender','trestbps','has_history','cp','chol','fbs','restecg'])

    prediction = knn.predict(user_features)

    predicted_class = dataset['heart_disease'][prediction].values[0]
    probabilities = knn.predict_proba(user_features)

    probability = round(probabilities[0][1]*100, 4)

    y_pred_test = knn.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred_test)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    precision = round(precision_score(y_test, y_pred_test)*100,4)
    f1 = round(f1_score(y_test, y_pred_test)*100,4)
    recall = round(recall_score(y_test, y_pred_test)*100,4)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = round(np.sqrt(mse),4)

    return predicted_class, probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

X = dataset[['age','gender','trestbps','has_history','cp','chol','fbs','restecg','thalach','thal']]
y = dataset['heart_disease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=12)
knn.fit(X_train, y_train)

# Use PCA to reduce dimensions to 2D for visualization
pca = PCA(n_components=2).fit(X)
pca_2d = pca.transform(X)

# Visualize the data
# for i in range(0, pca_2d.shape[0]):
#     if y[i] == 0:
#         c1 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='b', marker='+')
#     elif y[i] == 1:
#         c2 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='r', marker='o')

# plt.legend([c1, c2], ['No Heart Disease','With Heart Disease'])
# plt.show()

# Support Vector Machine (SVM) Code for Preliminary Inputs

def SVMPre(age, gender, trestbps, has_history, cp):
    X = dataset[['age','gender','trestbps','has_history','cp']]
    y = dataset['heart_disease']


    # Split the dataset into a training set and a test set
    X_train, X_test, y_train, y_test = train_test_split(X
                                                        , y
                                                        , test_size=0.2
                                                        , random_state=42)

    # Create the SVM classifier
    svm_model = SVC(kernel='linear', probability=True)

    # Train the classifier on the training data
    svm_model.fit(X_train, y_train)

    # Get user measurements
    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp]],
                                columns = ['age','gender','trestbps','has_history','cp'])


    # Use the model to predict the class of the flower with the user's values
    prediction = svm_model.predict(user_features)
    probabilities = svm_model.predict_proba(user_features)
    probability = round(probabilities[0,1]*100, 4)

    # Optionally, provide accuracy information on the test set
    y_pred = svm_model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred)
    precision = round(precision_score(y_test, y_pred)*100,4)
    f1 = round(f1_score(y_test, y_pred)*100,4)
    recall = round(recall_score(y_test, y_pred)*100,4)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    # Convert the probability from a range of -1 to 1 to a range of 0% to 100%

    return prediction[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# Support Vector Machine (SVM) Code for Above 35 Probability

def SVMAbove35(age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal):
    X = dataset[['age','gender','trestbps','has_history','cp','chol','fbs','restecg','thalach','thal']]
    y = dataset['heart_disease']

    # Split the dataset into a training set and a test set
    X_train, X_test, y_train, y_test = train_test_split(X
                                                        , y
                                                        , test_size=0.2
                                                        , random_state=42)

    # Create the SVM classifier
    svm_model = SVC(kernel='linear', probability = True)

    # Train the classifier on the training data
    svm_model.fit(X_train, y_train)

    # Get user measurements
    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg, thalach, thal]],
                                columns = ['age','gender','trestbps','has_history','cp','chol','fbs','restecg','thalach','thal'])

    # Use the model to predict the class of the flower with the user's values
    prediction = svm_model.predict(user_features)
    probabilities = svm_model.predict_proba(user_features)
    probability = round(probabilities[0,1]*100, 4)

    # Optionally, provide accuracy information on the test set
    y_pred = svm_model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred)
    precision = round(precision_score(y_test, y_pred)*100,4)
    f1 = round(f1_score(y_test, y_pred)*100,4)
    recall = round(recall_score(y_test, y_pred)*100,4)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    feature_name = X
    feature_names = feature_name.columns.tolist()

    # Standardize the dataset
    scaler = StandardScaler()
    X_std = scaler.fit_transform(X)

    # Create a combination of all pairs of features
    feature_combinations = combinations(range(X.shape[1]), 2)

    # SVM classifier with a linear kernel
    svm = SVC(kernel='linear', C=1.0)

    # Plotting configuration with increased figure size and label sizes
    fig, axs = plt.subplots(5,9, figsize=(20, 15))  # Increased figure size here
    fig.subplots_adjust(hspace=0.4, wspace=0.4)
    axs = axs.flatten()  # To iterate over subplots easily

    # Loop over combinations of features
    for i, (feature_idx1, feature_idx2) in enumerate(feature_combinations):
        # Prepare data for the two features
        X_pair_std = X_std[:, [feature_idx1, feature_idx2]]

        # Train SVM
        svm.fit(X_pair_std, y)

        # Create a mesh to plot the decision boundaries
        x_min, x_max = X_pair_std[:, 0].min() - 1, X_pair_std[:, 0].max() + 1
        y_min, y_max = X_pair_std[:, 1].min() - 1, X_pair_std[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                            np.arange(y_min, y_max, 0.02))

        # Plot the decision boundary
        Z = svm.predict(np.array([xx.ravel(), yy.ravel()]).T)
        Z = Z.reshape(xx.shape)

        # Plotting the decision boundaries
        axs[i].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)
        axs[i].scatter(X_pair_std[:, 0], X_pair_std[:, 1], c=y, s=30, edgecolor='k', cmap=plt.cm.coolwarm)
        axs[i].set_xlabel(feature_names[feature_idx1], fontsize=14)  # Increase font size for x label
        axs[i].set_ylabel(feature_names[feature_idx2], fontsize=14)  # Increase font size for y label
        axs[i].tick_params(axis='both', which='major', labelsize=12)  # Increase tick label size
        axs[i].set_title(' ', fontsize=16)  # Increase title font size

    # Hide any empty subplot (in case the number of combinations is less than the subplot grid)
    for j in range(i + 1, len(axs)):
        axs[j].axis('off')

    # Download the visualization as png
    plt.savefig("assets/visualizations/svmVisual.png")

    return prediction[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse

# Support Vector Machine (SVM) Code for Below 35 Probability

def SVMBelow35(age, gender, trestbps, has_history, cp, chol, fbs, restecg):
    X = dataset[['age','gender','trestbps','has_history','cp','chol','fbs','restecg']]
    y = dataset['heart_disease']

    # Split the dataset into a training set and a test set
    X_train, X_test, y_train, y_test = train_test_split(X
                                                        , y
                                                        , test_size=0.2
                                                        , random_state=42)

    # Create the SVM classifier
    svm_model = SVC(kernel='linear', probability = True)

    # Train the classifier on the training data
    svm_model.fit(X_train, y_train)

    # Get user measurements
    user_features = pd.DataFrame([[age, gender, trestbps, has_history, cp, chol, fbs, restecg]],
                                columns = ['age','gender','trestbps','has_history','cp','chol','fbs','restecg'])

    # Use the model to predict the class of the flower with the user's values
    prediction = svm_model.predict(user_features)
    probabilities = svm_model.predict_proba(user_features)
    probability = round(probabilities[0][1]*100, 4)

    # Optionally, provide accuracy information on the test set
    y_pred = svm_model.predict(X_test)
    accuracy = round(accuracy_score(y_test, y_pred)*100,4)
    conf_matrix = confusion_matrix(y_test, y_pred)
    precision = round(precision_score(y_test, y_pred)*100,4)
    f1 = round(f1_score(y_test, y_pred)*100,4)
    recall = round(recall_score(y_test, y_pred)*100,4)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    feature_name = X
    feature_names = feature_name.columns.tolist()

    # Standardize the dataset
    scaler = StandardScaler()
    X_std = scaler.fit_transform(X)

    # Create a combination of all pairs of features
    feature_combinations = combinations(range(X.shape[1]), 2)

    # SVM classifier with a linear kernel
    svm = SVC(kernel='linear', C=1.0)

    # Plotting configuration with increased figure size and label sizes
    fig, axs = plt.subplots(5,6, figsize=(20, 15))  # Increased figure size here
    fig.subplots_adjust(hspace=0.4, wspace=0.4)
    axs = axs.flatten()  # To iterate over subplots easily

    # Loop over combinations of features
    for i, (feature_idx1, feature_idx2) in enumerate(feature_combinations):
        # Prepare data for the two features
        X_pair_std = X_std[:, [feature_idx1, feature_idx2]]

        # Train SVM
        svm.fit(X_pair_std, y)

        # Create a mesh to plot the decision boundaries
        x_min, x_max = X_pair_std[:, 0].min() - 1, X_pair_std[:, 0].max() + 1
        y_min, y_max = X_pair_std[:, 1].min() - 1, X_pair_std[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                            np.arange(y_min, y_max, 0.02))

        # Plot the decision boundary
        Z = svm.predict(np.array([xx.ravel(), yy.ravel()]).T)
        Z = Z.reshape(xx.shape)

        # Plotting the decision boundaries
        axs[i].contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)
        axs[i].scatter(X_pair_std[:, 0], X_pair_std[:, 1], c=y, s=30, edgecolor='k', cmap=plt.cm.coolwarm)
        axs[i].set_xlabel(feature_names[feature_idx1], fontsize=14)  # Increase font size for x label
        axs[i].set_ylabel(feature_names[feature_idx2], fontsize=14)  # Increase font size for y label
        axs[i].tick_params(axis='both', which='major', labelsize=12)  # Increase tick label size
        axs[i].set_title(' ', fontsize=16)  # Increase title font size

    # Hide any empty subplot (in case the number of combinations is less than the subplot grid)
    for j in range(i + 1, len(axs)):
        axs[j].axis('off')

    # Download the visualization as png
    plt.savefig("assets/visualizations/svmVisual.png")

    return prediction[0], probability, accuracy, conf_matrix, precision, f1, recall, mse, rmse


